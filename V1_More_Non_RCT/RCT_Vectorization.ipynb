{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dc/anaconda3/envs/mxnet/lib/python3.6/site-packages/scipy/sparse/sparsetools.py:20: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import multiprocessing\n",
    "import time\n",
    "import csv\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "import warnings\n",
    "import tqdm\n",
    "import logging\n",
    "import mxnet as mx\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from os.path import basename\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "%pylab inline\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingTable = pd.read_csv('trainingTable.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMCID</th>\n",
       "      <th>RCT</th>\n",
       "      <th>PMCFILE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PMC29099</td>\n",
       "      <td>False</td>\n",
       "      <td>PMC29099.nxml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PMC29105</td>\n",
       "      <td>False</td>\n",
       "      <td>PMC29105.nxml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PMC32159</td>\n",
       "      <td>False</td>\n",
       "      <td>PMC32159.nxml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PMC32179</td>\n",
       "      <td>False</td>\n",
       "      <td>PMC32179.nxml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PMC35278</td>\n",
       "      <td>False</td>\n",
       "      <td>PMC35278.nxml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      PMCID    RCT        PMCFILE\n",
       "0  PMC29099  False  PMC29099.nxml\n",
       "1  PMC29105  False  PMC29105.nxml\n",
       "2  PMC32159  False  PMC32159.nxml\n",
       "3  PMC32179  False  PMC32179.nxml\n",
       "4  PMC35278  False  PMC35278.nxml"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingTable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest = 'RCTData/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X = []\n",
    "data_Y = []\n",
    "\n",
    "# Load stop-words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(('name','surname','given','first'))\n",
    "\n",
    "# Initialize tokenizer\n",
    "regexp_tokenizer = RegexpTokenizer('[\\'a-zA-Z]+')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a document to words\n",
    "def tokenize(document, rebuild_document=True):\n",
    "    words = []\n",
    "\n",
    "    for sentence in sent_tokenize(document):\n",
    "        tokens = [wordnet_lemmatizer.lemmatize(t.lower()) \n",
    "                  for t in regexp_tokenizer.tokenize(sentence) if t.lower() not in stop_words]\n",
    "        words += tokens\n",
    "\n",
    "    if rebuild_document:\n",
    "        return ' '.join(words).strip()\n",
    "    else:\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19111/19111 [43:46<00:00,  7.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# put all tokens into dataset data_X and assign the correspond labels\n",
    "for i in tqdm.tqdm(range(len(trainingTable['PMCID']))):\n",
    "    fileName = os.path.join(dest, trainingTable['PMCFILE'][i])\n",
    "    with open(fileName, 'r',encoding='ISO-8859-1') as data_file:\n",
    "        rawdata = data_file.read()\n",
    "    soup = BeautifulSoup(rawdata)\n",
    "    text = soup.get_text()\n",
    "    data_X.append(tokenize(text))\n",
    "    data_Y.append(trainingTable['RCT'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_file = 'data_X'\n",
    "\n",
    "with open(p_file, 'wb') as fout:\n",
    "    pickle.dump(data_X, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tagged documents\n",
    "taggedDoc = []\n",
    "\n",
    "for i, document in enumerate(data_X):\n",
    "    taggedDoc.append(TaggedDocument(document.split(' '), [i]))\n",
    "\n",
    "p_file = 'taggedDoc'\n",
    "with open(p_file, 'wb') as fout:\n",
    "    pickle.dump(taggedDoc, fout)\n",
    "\n",
    "# Train Doc2Vec model\n",
    "# https://arxiv.org/pdf/1405.4053v2.pdf\n",
    "doc2vecSize = 32*32\n",
    "\n",
    "doc2vecModel = Doc2Vec(documents=taggedDoc, size=doc2vecSize, window=3, \n",
    "                        min_count=2, iter=30, workers=multiprocessing.cpu_count())\n",
    "doc2vecModel.save('doc2vecModel')\n",
    "doc2vecModel.init_sims(replace=False)\n",
    "\n",
    "# Vectorize documents and split corpus to training and testing\n",
    "vectors = []\n",
    "\n",
    "for document in data_X:\n",
    "    vectors.append(doc2vecModel.infer_vector(document.split(' ')))\n",
    "\n",
    "d2v_X_train, d2v_X_test, d2v_Y_train, d2v_Y_test = train_test_split(vectors, data_Y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_array = np.array(d2v_X_train)\n",
    "Y_train_array = np.array(d2v_Y_train)\n",
    "X_test_array = np.array(d2v_X_test)\n",
    "Y_test_array = np.array(d2v_Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_file = 'RCT_Vectors'\n",
    "\n",
    "with open(p_file, 'wb') as fout:\n",
    "    pickle.dump(vectors, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_file = 'RCT_labels'\n",
    "\n",
    "with open(p_file, 'wb') as fout:\n",
    "    pickle.dump(data_Y, fout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MXNET",
   "language": "python",
   "name": "mxnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
