{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- The DOC2VEC is trained on window=21, word_count =1, and iter =50 to generate 16x16 size vectors\n",
    "- This notebook is not very time consuming. It runs about 1 hour in my desktop (2 XEON and 40G DDR)\n",
    "- During processing, this notebook consumes about 10G memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dc/anaconda3/envs/mxnet/lib/python3.6/site-packages/scipy/sparse/sparsetools.py:20: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import multiprocessing\n",
    "import time\n",
    "import csv\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "import warnings\n",
    "import tqdm\n",
    "import logging\n",
    "import mxnet as mx\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from os.path import basename\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "%pylab inline\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load stop-words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(('name','surname','given','first'))\n",
    "\n",
    "# Initialize tokenizer\n",
    "regexp_tokenizer = RegexpTokenizer('[\\'a-zA-Z]+')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a document to words\n",
    "def tokenize(document, rebuild_document=True):\n",
    "    words = []\n",
    "\n",
    "    for sentence in sent_tokenize(document):\n",
    "        tokens = [wordnet_lemmatizer.lemmatize(t.lower()) \n",
    "                  for t in regexp_tokenizer.tokenize(sentence) if t.lower() not in stop_words]\n",
    "        words += tokens\n",
    "\n",
    "    if rebuild_document:\n",
    "        return ' '.join(words).strip()\n",
    "    else:\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenized file\n",
    "p_file = 'taggedDoc'\n",
    "with open(p_file, 'rb') as fin:\n",
    "    taggedDoc = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time consumed to train the Doc2Vec:  4157.049939393997\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Train Doc2Vec model\n",
    "# https://arxiv.org/pdf/1405.4053v2.pdf\n",
    "doc2vecSize = 256\n",
    "\n",
    "doc2vecModel = Doc2Vec(documents=taggedDoc, size=doc2vecSize, window=21, \n",
    "                        min_count=1, iter=50, workers=multiprocessing.cpu_count())\n",
    "doc2vecModel.save('doc2vecModel')\n",
    "doc2vecModel.init_sims(replace=False)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print (\"Time consumed to train the Doc2Vec: \", elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_file = 'data_X'\n",
    "\n",
    "with open(p_file, 'rb') as fin:\n",
    "    data_X = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19111/19111 [26:57<00:00, 11.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Vectorize documents\n",
    "vectors = []\n",
    "\n",
    "for i in tqdm.tqdm(range(len(data_X))):\n",
    "    vectors.append(doc2vecModel.infer_vector(data_X[i].split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_file = 'RCT_Vectors'\n",
    "\n",
    "with open(p_file, 'wb') as fout:\n",
    "    pickle.dump(vectors, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MXNET",
   "language": "python",
   "name": "mxnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
